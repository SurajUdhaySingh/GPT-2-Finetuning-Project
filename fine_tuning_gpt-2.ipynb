{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's mention some key points.\n",
    "1. We will use Hugging Face's Transformers, and its training and datasets API.\n",
    "2. For training, CUDA 12.4 will be used. It's compatible with current PyTorch version. Refer to PyTorch docs for a table of compatible CUDA and PyTorch versions. Moreover, is isn't neccassary to import 'torch' and define 'device' variable, i.e, it is optional to manually set CUDA device to the model, and dataset, as Hugging Face Transformers will do it for us.\n",
    "3. The training data is tiny, with just above 1000 samples, and far from being a corpus. Nonetheless, this project is for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel # Import GPT-2's LM head and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"gpt2\"\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(model_ckpt)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token # Make sure we manually set the 'pad_token.'\n",
    "gpt2_lmheadmodel = GPT2LMHeadModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# Set the model to evaluation mode.\n",
    "gpt2_lmheadmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see GPT-2's structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text from a prompt\n",
    "\n",
    "prompt = \"World War I was \"\n",
    "input_ids = gpt2_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "gen_text = gpt2_lmheadmodel.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=gpt2_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin fine-tuning, let's see how base GPT-2 performs. Here, we use some parameters do efficiently decode the outputs. Here's an explanation of all these parameters:\n",
    "\n",
    "max_length=128\t=> Sets the maximum length of the generated sequence (including the input prompt).\n",
    "\n",
    "num_return_sequences=1 =>\tSpecifies the number of sequences to generate. Here, we generate only one output.\n",
    "\n",
    "no_repeat_ngram_size=2 =>\tPrevents repeating n-grams (subsequences of words) of size 2 or more, reducing repetitive text.\n",
    "\n",
    "do_sample=True =>\tEnables sampling instead of greedy decoding, making the output more diverse and creative.\n",
    "\n",
    "top_k=50 =>\tUses Top-K sampling, where only the top 50 most probable words are considered at each step.\n",
    "\n",
    "top_p=0.95 =>\tUses nucleus sampling (Top-P sampling), where only words with a cumulative probability of 95% are considered. This makes generation more dynamic.\n",
    "\n",
    "temperature=0.7 =>\tControls creativity. Lower values (e.g., 0.5) make the model more conservative, while higher values (e.g., 1.2) make it more random.\n",
    "\n",
    "pad_token_id=gpt2_tokenizer.eos_token_id =>\tEnsures proper padding by using the end-of-sequence (EOS) token for GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 says: \n",
      "World War I was Â an era when the military and industrial powers in the Middle East were in power, and those forces were using nuclear weapons to control populations.\n",
      "In the 1950s, when I began my research on nuclear energy, I realized that the most effective way to use nuclear power was to build a nuclear reactor. I quickly learned that there was no nuclear fuel, that nuclear reactors were not used to produce energy in a vacuum, but to create a mass of waste that would then be used for nuclear fuels. It took me many years to develop a plan for a new type of nuclear waste storage facility in New York City,\n"
     ]
    }
   ],
   "source": [
    "gen_text_decoded = gpt2_tokenizer.decode(\n",
    "    gen_text[0], skip_special_tokens=True\n",
    ")\n",
    "print(f'GPT-2 says: \\n{gen_text_decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text, to say the least, is inaccurate, and seemingly a bunch of WW1 terminology thrown and put together. One thing we can infer, is that its grammatical and morphological structure is sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia article was extracted from 'World War I' and saved to 'World War I.txt'\n",
      "Wikipedia article was extracted from 'Battle of Verdun' and saved to 'Battle of Verdun.txt'\n",
      "Wikipedia article was extracted from 'Schlieffen Plan' and saved to 'Schlieffen Plan.txt'\n",
      "Wikipedia article was extracted from 'Trench warfare' and saved to 'Trench warfare.txt'\n",
      "Wikipedia article was extracted from 'Western Front (World War I)' and saved to 'Western Front (World War I).txt'\n",
      "Wikipedia article was extracted from 'Eastern Front (World War I)' and saved to 'Eastern Front (World War I).txt'\n",
      "Wikipedia article was extracted from 'Treaty of Versailles' and saved to 'Treaty of Versailles.txt'\n",
      "Wikipedia article was extracted from 'Battle of the Somme' and saved to 'Battle of the Somme.txt'\n",
      "Wikipedia article was extracted from 'Armistice of 11 November 1918' and saved to 'Armistice of 11 November 1918.txt'\n",
      "Wikipedia article was extracted from 'Gallipoli campaign' and saved to 'Gallipoli campaign.txt'\n",
      "Wikipedia article was extracted from 'Battle of Jutland' and saved to 'Battle of Jutland.txt'\n",
      "Wikipedia article was extracted from 'Zimmermann Telegram' and saved to 'Zimmermann Telegram.txt'\n",
      "Wikipedia article was extracted from 'Russian Revolution' and saved to 'Russian Revolution.txt'\n",
      "Wikipedia article was extracted from 'Central Powers' and saved to 'Central Powers.txt'\n",
      "Wikipedia article was extracted from 'Allied Powers (World War I)' and saved to 'Allied Powers (World War I).txt'\n"
     ]
    }
   ],
   "source": [
    "# To improve GPT-2's performance, we will extract data from Wikipedia, using web_scrape_utility.py\n",
    "\n",
    "from web_scrape_utility import extract_text_from_wiki\n",
    "\n",
    "titles = [\n",
    "    \"World War I\",\n",
    "    \"Battle of Verdun\",\n",
    "    \"Schlieffen Plan\",\n",
    "    \"Trench warfare\",\n",
    "    \"Western Front (World War I)\",\n",
    "    \"Eastern Front (World War I)\",\n",
    "    \"Treaty of Versailles\",\n",
    "    \"Battle of the Somme\",\n",
    "    \"Armistice of 11 November 1918\",\n",
    "    \"Gallipoli campaign\",\n",
    "    \"Battle of Jutland\",\n",
    "    \"Zimmermann Telegram\",\n",
    "    \"Russian Revolution\",\n",
    "    \"Central Powers\",\n",
    "    \"Allied Powers (World War I)\"\n",
    "]\n",
    "\n",
    "for title in titles:\n",
    "    extract_text_from_wiki(title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's begin with first step. We will extract text from Wikipedia articles. Wikipedia provides numerous articles which can be scraped easily. Here, web_scrape_utility.py is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 15 articles from Wikipedia.\n"
     ]
    }
   ],
   "source": [
    "print(f'Extracted text from {len(titles)} articles from Wikipedia.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended World War I.txt to ww1_corpus.txt\n",
      "Appended Battle of Verdun.txt to ww1_corpus.txt\n",
      "Appended Schlieffen Plan.txt to ww1_corpus.txt\n",
      "Appended Trench warfare.txt to ww1_corpus.txt\n",
      "Appended Western Front (World War I).txt to ww1_corpus.txt\n",
      "Appended Eastern Front (World War I).txt to ww1_corpus.txt\n",
      "Appended Treaty of Versailles.txt to ww1_corpus.txt\n",
      "Appended Battle of the Somme.txt to ww1_corpus.txt\n",
      "Appended Armistice of 11 November 1918.txt to ww1_corpus.txt\n",
      "Appended Gallipoli campaign.txt to ww1_corpus.txt\n",
      "Appended Battle of Jutland.txt to ww1_corpus.txt\n",
      "Appended Zimmermann Telegram.txt to ww1_corpus.txt\n",
      "Appended Russian Revolution.txt to ww1_corpus.txt\n",
      "Appended Central Powers.txt to ww1_corpus.txt\n",
      "Appended Allied Powers (World War I).txt to ww1_corpus.txt\n",
      "Corpus successfully created in ww1_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List of titles from your previous code\n",
    "titles = [\n",
    "    \"World War I\",\n",
    "    \"Battle of Verdun\",\n",
    "    \"Schlieffen Plan\",\n",
    "    \"Trench warfare\",\n",
    "    \"Western Front (World War I)\",\n",
    "    \"Eastern Front (World War I)\",\n",
    "    \"Treaty of Versailles\",\n",
    "    \"Battle of the Somme\",\n",
    "    \"Armistice of 11 November 1918\",\n",
    "    \"Gallipoli campaign\",\n",
    "    \"Battle of Jutland\",\n",
    "    \"Zimmermann Telegram\",\n",
    "    \"Russian Revolution\",\n",
    "    \"Central Powers\",\n",
    "    \"Allied Powers (World War I)\"\n",
    "]\n",
    "\n",
    "# Output file for the corpus\n",
    "corpus_file = \"ww1_corpus.txt\"\n",
    "\n",
    "try:\n",
    "    # Open the corpus file in write mode\n",
    "    with open(corpus_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        # Iterate through each title\n",
    "        for title in titles:\n",
    "            input_file = f\"{title}.txt\"\n",
    "            if os.path.exists(input_file):\n",
    "                # Read the content of the individual file\n",
    "                with open(input_file, \"r\", encoding=\"utf-8\") as input_txt:\n",
    "                    content = input_txt.read()\n",
    "                    # Write the title as a header (optional) and the content\n",
    "                    output.write(f\"\\n\\n=== {title} ===\\n\\n\")  # Separator for clarity\n",
    "                    output.write(content)\n",
    "                print(f\"Appended {input_file} to {corpus_file}\")\n",
    "            else:\n",
    "                print(f\"Warning: {input_file} not found, skipping...\")\n",
    "\n",
    "    print(f\"Corpus successfully created in {corpus_file}\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"Error creating corpus: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we compiled all the .txt files in a single 'corpus,' which I admit, is no corpus by any means, so that we can feed it to GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c7543b90044565ac0b5a8668433307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 2792 examples\n",
      "First example:\n",
      "{'text': ''}\n",
      "Number of lines in corpus: 2792\n",
      "Sample line:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "corpus_file = \"ww1_corpus.txt\"\n",
    "\n",
    "# Load the text file as a dataset\n",
    "dataset = load_dataset(\n",
    "    \"text\",  \n",
    "    data_files=corpus_file,\n",
    "    split=\"train\"  # Load as a single \"train\" split\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "print(\"First example:\")\n",
    "print(dataset[0])\n",
    "corpus_text = dataset[\"text\"]  # List of lines from the file\n",
    "print(f\"Number of lines in corpus: {len(corpus_text)}\")\n",
    "print(\"Sample line:\")\n",
    "print(corpus_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a number of classes and functions, generously provided by Hugging Face. To name them, we'd use:\n",
    "1. load_dataset(), to create a dataset, it supports .txt files!\n",
    "2. GPT2Tokenizer and GPT2LMHeadModel, which we've imported already.\n",
    "3. TrainingArguments to define, well, training arguments.\n",
    "4. Trainer, which carries out the fine-tuning/ training process.\n",
    "5. DataCollatorForLanguageModelling class.\n",
    "\n",
    "Moreover, load_dataset returns a DatasetDict class, which has some pretty useful attributes and methods. But above, we see a problem, which is that some examples in the dataset are just empty. So, we'd need to deal with it by removing the empty samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f454535e0441adb4aae65199c1b4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset has 1441 examples\n",
      "First example: {'text': '=== World War I ==='}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "corpus_file = \"ww1_corpus.txt\"\n",
    "dataset = load_dataset(\"text\", data_files=corpus_file, split=\"train\")\n",
    "\n",
    "# Filter out empty lines and save to a new file\n",
    "cleaned_file = \"ww1_corpus_cleaned.txt\"\n",
    "with open(cleaned_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in dataset[\"text\"]:\n",
    "        if line.strip():  # Only write non-empty lines\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "# Reload the cleaned dataset\n",
    "dataset = load_dataset(\"text\", data_files=cleaned_file, split=\"train\")\n",
    "print(f\"Cleaned dataset has {len(dataset)} examples\")\n",
    "print(\"First example:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1441\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== World War I ===\n",
      "\n",
      "World War I[b] or the First World War (28 July 1914 â 11 November 1918), also known as the Great War, was a global conflict between two coalitions: the Allies (or Entente) and the Central Powers. Fighting took place mainly in Europe and the Middle East, as well as in parts of Africa and the Asia-Pacific, and in Europe was characterised by trench warfare; the widespread use of artillery, machine guns, and chemical weapons (gas); and the introductions of tanks and aircraft. World War I was one of the deadliest conflicts in history, resulting in an estimated 10 million military dead and more than 20 million wounded, plus some 10 million civilian dead from causes including genocide. The movement of large numbers of people was a major factor in the deadly Spanish flu pandemic.\n",
      "\n",
      "The causes of World War I included the rise of Germany and decline of the Ottoman Empire, which disturbed the long-standing balance of power in Europe, as well as economic competition between nations triggered by industrialisation and imperialism. Growing tensions between the great powers and in the Balkans reached a breaking point on 28 June 1914, when a Bosnian Serb named Gavrilo Princip assassinated Archduke Franz Ferdinand, heir to the Austro-Hungarian throne. Austria-Hungary held Serbia responsible, and declared war on 28 July. After Russia mobilised in Serbia's defence, Germany declared war on Russia and France, who had an alliance. The United Kingdom entered after Germany invaded Belgium, whose neutrality it guaranteed, and the Ottomans joined the Central Powers in November. Germany's strategy in 1914 was to quickly defeat France then transfer its forces to the east, but its advance was halted in September, and by the end of the year the Western Front consisted of a continuous line of trenches stretching from the English Channel to Switzerland. The Eastern Front was more dynamic, but neither side gained a decisive advantage, despite costly offensives. Italy, Bulgaria, Romania, Greece and others joined in from 1915 onward. \n",
      "\n",
      "Major battles, including at Verdun, the Somme, and Passchendaele, failed to break the stalemate on the Western Front. In April 1917, the United States joined the Allies after Germany resumed unrestricted submarine warfare against Atlantic shipping. Later that year, the Bolsheviks seized power in Russia in the October Revolution; Soviet Russia signed an armistice with the Central Powers in December, followed by a separate peace in March 1918. That month, Germany launched a spring offensive in the west, which despite initial successes left the German Army exhausted and demoralised. The Allied Hundred Days Offensive beginning in August 1918 caused a collapse of the German front line. By early November, Bulgaria, the Ottoman Empire and Austria-Hungary had each signed armistices with the Allies, leaving Germany isolated. Facing a revolution at home, Kaiser Wilhelm II abdicated on 9 November, and the war ended with the Armistice of 11 November 1918.\n",
      "\n",
      "The Paris Peace Conference of 1919â1920 imposed settlements on the defeated powers, most notably the Treaty of Versailles, by which Germany lost significant territories, was disarmed, and was required to pay large war reparations to the Allies. The dissolution of the Russian, German, Austro-Hungarian, and Ottoman Empires redrew national boundaries and resulted in the creation of new independent states, including Poland, Finland, the Baltic states, Czechoslovakia, and Yugoslavia. The League of Nations was established to maintain world peace, but its failure to manage instability during the interwar period contributed to the outbreak of World War II in 1939.\n",
      "\n",
      "Before World War II, the events of 1914â1918 were generally known as the Great War or simply the World War.[1] In August 1914, the magazine The Independent wrote \"This is the Great War. It names itself\".[2] In October 1914, the Canadian magazine Maclean's similarly wrote, \"Some wars name themselves. This is the Great War.\"[3] Contemporary Europeans also referred to it as \"the war to end war\" and it was also described as \"the war to end all wars\" due to their perception of its unparalleled scale, devastation, and loss of life.[4] The first recorded use of the term First World War was in September 1914 by German biologist and philosopher Ernst Haeckel who stated, \"There is no doubt that the course and character of the feared 'European War'Â ... will become the first world war in the full sense of the word.\"[5]\n",
      "\n",
      "For much of the 19th century, the major European powers maintained a tenuous balance of power, known as the Concert of Europe.[6] After 1848, this was challenged by Britain's withdrawal into so-called splendid isolation, the decline of the Ottoman Empire, New Imperialism, and the rise of Prussia under Otto von Bismarck. Victory in the 1870â1871 Franco-Prussian War allowed Bismarck to consolidate a German Empire. Post-1871, the primary aim of French policy was to avenge this defeat,[7] but by the early 1890s, this had switched to the expansion of the French colonial empire.[8]\n",
      "\n",
      "In 1873, Bismarck negotiated the League of the Three Emperors, which included Austria-Hungary, Russia, and Germany. After the 1877â1878 Russo-Turkish War, the League was dissolved due to Austrian concerns over the expansion of Russian influence in the Balkans, an area they considered to be of vital strategic interest. Germany and Austria-Hungary then formed the 1879 Dual Alliance, which became the Triple Alliance when Italy joined in 1882.[9] For Bismarck, the purpose of these agreements was to isolate France by ensuring the three empires resolved any disputes between themselves. In 1887, Bismarck set up the Reinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.[10]\n",
      "\n",
      "For Bismarck, peace with Russia was the foundation of German foreign policy, but in 1890, he was forced to retire by Wilhelm II. The latter was persuaded not to renew the Reinsurance Treaty by his new Chancellor, Leo von Caprivi.[11] This gave France an opening to agree to the Franco-Russian Alliance in 1894, which was then followed by the 1904 Entente Cordiale with Britain. The Triple Entente was completed by the 1907 Anglo-Russian Convention. While not formal alliances, by settling longstanding colonial disputes in Asia and Africa, British support for France or Russia in any future conflict became a possibility.[12] This was accentuated by British and Russian support for France against Germany during the 1911 Agadir Crisis.[13]\n",
      "\n",
      "German economic and industrial strength continued to expand rapidly post-1871. Backed by Wilhelm II, Admiral Alfred von Tirpitz sought to use this growth to build an Imperial German Navy, that could compete with the British Royal Navy.[14] This policy was based on the work of US naval author Alfred Thayer Mahan, who argued that possession of a blue-water navy was vital for global power projection; Tirpitz had his books translated into German, while Wilhelm made them required reading for his advisors and senior military personnel.[15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in dataset['text'][:10]:\n",
    "    print(example, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like WW1 history alright! Now, let's see an example of how GPT-2's tokenizer converts raw text into token IDs, which are numerical representations of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10603,  1810,   314,    58,    65,    60,   393,   262,  3274,  2159,\n",
       "          1810,   357,  2078,  2901, 26833,   784,  1367,  3389, 25859,   828,\n",
       "           635,  1900,   355,   262,  3878,  1810,    11,   373,   257,  3298,\n",
       "          5358,  1022,   734,  5655,  1756,    25,   262, 32430,   357,   273,\n",
       "          7232, 21872,     8,   290,   262,  5694, 20668,    13, 19098,  1718,\n",
       "          1295,  8384,   287,  2031,   290,   262,  6046,  3687,    11,   355,\n",
       "           880,   355,   287,  3354,   286,  5478,   290,   262,  7229,    12,\n",
       "         22933,    11,   290,   287,  2031,   373,  2095,  1417,   416, 35091,\n",
       "         15611,    26,   262, 10095,   779,   286, 20381,    11,  4572,  6541,\n",
       "            11,   290,  5931,  3777,   357, 22649,  1776,   290,   262,  3120,\n",
       "          2733,   286, 11657,   290,  6215,    13,  2159,  1810,   314,   373,\n",
       "           530,   286,   262, 39268, 12333,   287,  2106,    11,  7186,   287,\n",
       "           281,  6108,   838,  1510,  2422,  2636,   290,   517,   621,  1160,\n",
       "          1510, 10657,    11,  5556,   617,   838,  1510, 11107,  2636,   422,\n",
       "          5640,  1390, 20744,    13,   383,  3356,   286,  1588,  3146,   286,\n",
       "           661,   373,   257,  1688,  5766,   287,   262, 10309,  7897,  6562,\n",
       "         19798,  5314,    13]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A glance at how does the GPT2 tokenizer does its job.\n",
    "\n",
    "sample = dataset['text'][1]\n",
    "sample_tokenized = gpt2_tokenizer.encode(sample, return_tensors=\"pt\")\n",
    "sample_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 10603\n",
      "Token: World\n",
      "Token ID: 1810\n",
      "Token:  War\n",
      "Token ID: 314\n",
      "Token:  I\n",
      "Token ID: 58\n",
      "Token: [\n",
      "Token ID: 65\n",
      "Token: b\n",
      "Token ID: 60\n",
      "Token: ]\n",
      "Token ID: 393\n",
      "Token:  or\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 3274\n",
      "Token:  First\n",
      "Token ID: 2159\n",
      "Token:  World\n",
      "Token ID: 1810\n",
      "Token:  War\n",
      "Token ID: 357\n",
      "Token:  (\n",
      "Token ID: 2078\n",
      "Token: 28\n",
      "Token ID: 2901\n",
      "Token:  July\n",
      "Token ID: 26833\n",
      "Token:  1914\n",
      "Token ID: 784\n",
      "Token:  â\n",
      "Token ID: 1367\n",
      "Token:  11\n",
      "Token ID: 3389\n",
      "Token:  November\n",
      "Token ID: 25859\n",
      "Token:  1918\n",
      "Token ID: 828\n",
      "Token: ),\n",
      "Token ID: 635\n",
      "Token:  also\n",
      "Token ID: 1900\n",
      "Token:  known\n",
      "Token ID: 355\n",
      "Token:  as\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 3878\n",
      "Token:  Great\n",
      "Token ID: 1810\n",
      "Token:  War\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 373\n",
      "Token:  was\n",
      "Token ID: 257\n",
      "Token:  a\n",
      "Token ID: 3298\n",
      "Token:  global\n",
      "Token ID: 5358\n",
      "Token:  conflict\n",
      "Token ID: 1022\n",
      "Token:  between\n",
      "Token ID: 734\n",
      "Token:  two\n",
      "Token ID: 5655\n",
      "Token:  coal\n",
      "Token ID: 1756\n",
      "Token: itions\n",
      "Token ID: 25\n",
      "Token: :\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 32430\n",
      "Token:  Allies\n",
      "Token ID: 357\n",
      "Token:  (\n",
      "Token ID: 273\n",
      "Token: or\n",
      "Token ID: 7232\n",
      "Token:  Ent\n",
      "Token ID: 21872\n",
      "Token: ente\n",
      "Token ID: 8\n",
      "Token: )\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 5694\n",
      "Token:  Central\n",
      "Token ID: 20668\n",
      "Token:  Powers\n",
      "Token ID: 13\n",
      "Token: .\n",
      "Token ID: 19098\n",
      "Token:  Fighting\n",
      "Token ID: 1718\n",
      "Token:  took\n",
      "Token ID: 1295\n",
      "Token:  place\n",
      "Token ID: 8384\n",
      "Token:  mainly\n",
      "Token ID: 287\n",
      "Token:  in\n",
      "Token ID: 2031\n",
      "Token:  Europe\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 6046\n",
      "Token:  Middle\n",
      "Token ID: 3687\n",
      "Token:  East\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 355\n",
      "Token:  as\n",
      "Token ID: 880\n",
      "Token:  well\n",
      "Token ID: 355\n",
      "Token:  as\n",
      "Token ID: 287\n",
      "Token:  in\n",
      "Token ID: 3354\n",
      "Token:  parts\n",
      "Token ID: 286\n",
      "Token:  of\n",
      "Token ID: 5478\n",
      "Token:  Africa\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 7229\n",
      "Token:  Asia\n",
      "Token ID: 12\n",
      "Token: -\n",
      "Token ID: 22933\n",
      "Token: Pacific\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 287\n",
      "Token:  in\n",
      "Token ID: 2031\n",
      "Token:  Europe\n",
      "Token ID: 373\n",
      "Token:  was\n",
      "Token ID: 2095\n",
      "Token:  character\n",
      "Token ID: 1417\n",
      "Token: ised\n",
      "Token ID: 416\n",
      "Token:  by\n",
      "Token ID: 35091\n",
      "Token:  trench\n",
      "Token ID: 15611\n",
      "Token:  warfare\n",
      "Token ID: 26\n",
      "Token: ;\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 10095\n",
      "Token:  widespread\n",
      "Token ID: 779\n",
      "Token:  use\n",
      "Token ID: 286\n",
      "Token:  of\n",
      "Token ID: 20381\n",
      "Token:  artillery\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 4572\n",
      "Token:  machine\n",
      "Token ID: 6541\n",
      "Token:  guns\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 5931\n",
      "Token:  chemical\n",
      "Token ID: 3777\n",
      "Token:  weapons\n",
      "Token ID: 357\n",
      "Token:  (\n",
      "Token ID: 22649\n",
      "Token: gas\n",
      "Token ID: 1776\n",
      "Token: );\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 3120\n",
      "Token:  introdu\n",
      "Token ID: 2733\n",
      "Token: ctions\n",
      "Token ID: 286\n",
      "Token:  of\n",
      "Token ID: 11657\n",
      "Token:  tanks\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 6215\n",
      "Token:  aircraft\n",
      "Token ID: 13\n",
      "Token: .\n",
      "Token ID: 2159\n",
      "Token:  World\n",
      "Token ID: 1810\n",
      "Token:  War\n",
      "Token ID: 314\n",
      "Token:  I\n",
      "Token ID: 373\n",
      "Token:  was\n",
      "Token ID: 530\n",
      "Token:  one\n",
      "Token ID: 286\n",
      "Token:  of\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 39268\n",
      "Token:  deadliest\n",
      "Token ID: 12333\n",
      "Token:  conflicts\n",
      "Token ID: 287\n",
      "Token:  in\n",
      "Token ID: 2106\n",
      "Token:  history\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 7186\n",
      "Token:  resulting\n",
      "Token ID: 287\n",
      "Token:  in\n",
      "Token ID: 281\n",
      "Token:  an\n",
      "Token ID: 6108\n",
      "Token:  estimated\n",
      "Token ID: 838\n",
      "Token:  10\n",
      "Token ID: 1510\n",
      "Token:  million\n",
      "Token ID: 2422\n",
      "Token:  military\n",
      "Token ID: 2636\n",
      "Token:  dead\n",
      "Token ID: 290\n",
      "Token:  and\n",
      "Token ID: 517\n",
      "Token:  more\n",
      "Token ID: 621\n",
      "Token:  than\n",
      "Token ID: 1160\n",
      "Token:  20\n",
      "Token ID: 1510\n",
      "Token:  million\n",
      "Token ID: 10657\n",
      "Token:  wounded\n",
      "Token ID: 11\n",
      "Token: ,\n",
      "Token ID: 5556\n",
      "Token:  plus\n",
      "Token ID: 617\n",
      "Token:  some\n",
      "Token ID: 838\n",
      "Token:  10\n",
      "Token ID: 1510\n",
      "Token:  million\n",
      "Token ID: 11107\n",
      "Token:  civilian\n",
      "Token ID: 2636\n",
      "Token:  dead\n",
      "Token ID: 422\n",
      "Token:  from\n",
      "Token ID: 5640\n",
      "Token:  causes\n",
      "Token ID: 1390\n",
      "Token:  including\n",
      "Token ID: 20744\n",
      "Token:  genocide\n",
      "Token ID: 13\n",
      "Token: .\n",
      "Token ID: 383\n",
      "Token:  The\n",
      "Token ID: 3356\n",
      "Token:  movement\n",
      "Token ID: 286\n",
      "Token:  of\n",
      "Token ID: 1588\n",
      "Token:  large\n",
      "Token ID: 3146\n",
      "Token:  numbers\n",
      "Token ID: 286\n",
      "Token:  of\n",
      "Token ID: 661\n",
      "Token:  people\n",
      "Token ID: 373\n",
      "Token:  was\n",
      "Token ID: 257\n",
      "Token:  a\n",
      "Token ID: 1688\n",
      "Token:  major\n",
      "Token ID: 5766\n",
      "Token:  factor\n",
      "Token ID: 287\n",
      "Token:  in\n",
      "Token ID: 262\n",
      "Token:  the\n",
      "Token ID: 10309\n",
      "Token:  deadly\n",
      "Token ID: 7897\n",
      "Token:  Spanish\n",
      "Token ID: 6562\n",
      "Token:  flu\n",
      "Token ID: 19798\n",
      "Token:  pand\n",
      "Token ID: 5314\n",
      "Token: emic\n",
      "Token ID: 13\n",
      "Token: .\n"
     ]
    }
   ],
   "source": [
    "for token_id in sample_tokenized[0]:\n",
    "    print(f'Token ID: {token_id}\\nToken: {gpt2_tokenizer.decode(token_id)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is assigned a number, based on the contexts, rather than random assignment. Indeed, this allosws language models to learn relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer function.\n",
    "\n",
    "def tokenizer_fxn(examples):\n",
    "    return gpt2_tokenizer(examples['text'], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4112dab47bf54a5292c7b033168a194e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the dataset, i.e. encode the training samples\n",
    "\n",
    "dataset_encoded = dataset.map(tokenizer_fxn, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset has been encoded. Next, we generate a train-test split, keeping ~ 10% dataset for testing. Then we define the hyper-parameters, and begin the process of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1296, test size: 145\n"
     ]
    }
   ],
   "source": [
    "# Generate the train-test split.\n",
    "\n",
    "train_test_split = dataset_encoded.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "print(f'Train size: {len(train_dataset)}, test size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator.\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments, and use Hugging Face's trainer API for fine-tuning.\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt-ww1-finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    warmup_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Define the trainer.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt2_lmheadmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1620 05:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.498000</td>\n",
       "      <td>3.363681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.204800</td>\n",
       "      <td>3.326559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.138800</td>\n",
       "      <td>3.302164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.992200</td>\n",
       "      <td>3.306558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.894500</td>\n",
       "      <td>3.306001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.800300</td>\n",
       "      <td>3.306996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.737400</td>\n",
       "      <td>3.317396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.728900</td>\n",
       "      <td>3.318511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-ww1-finetuned\\\\tokenizer_config.json',\n",
       " './gpt2-ww1-finetuned\\\\special_tokens_map.json',\n",
       " './gpt2-ww1-finetuned\\\\vocab.json',\n",
       " './gpt2-ww1-finetuned\\\\merges.txt',\n",
       " './gpt2-ww1-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by calling .train(), and save the model.\n",
    "trainer.train()\n",
    "trainer.save_model('./gpt2-ww1-finetuned')\n",
    "gpt2_tokenizer.save_pretrained('./gpt2-ww1-finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss decreases consistently, while validation loss hovers around ~ 3.30. Our model has certainly learned something! With our training done, let's proceed to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "World War I was a significant event in European history, and the war saw two major revolutions: one by Germany's Allies against France (1914â18), the other as an attempt to end World Wars II. The first revolution involved Russia seizing power from Austria-Hungary; later that year Britain declared victory over French rule.[43] In 1914 after Franco-British support for British expansionism faltered, the Bolsheviks overthrew Tsar Nicholas V in 1917 with Russian backing. However this did not last long; many revolutionaries died before they could be replaced by their own party or political movement.[44][45]:[46]) This led both sides towards civil wars which continued into 1918,[47]. Although there were some successes such at Verdun, these resulted mainly of violent clashes between combatants â usually unarmed protestors who attempted nonviolent protests but became increasingly isolated due either lacklustre leadership skills or poor organisation.[48] During the Second Battle of Ypres, several thousand soldiers took part in demonstrations demanding\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./gpt2-ww1-finetuned\")\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-ww1-finetuned\")\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"World War I was\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,  # Increased length for better context\n",
    "    do_sample=True,  # Sampling for diverse output\n",
    "    top_k=40,        # Reduce randomness slightly (more controlled than 50)\n",
    "    top_p=0.92,      # Slightly lower nucleus sampling to reduce extreme randomness\n",
    "    temperature=0.65, # Lowered to balance creativity and coherence\n",
    "    repetition_penalty=1.2,  # Reduces looping and repeated phrases\n",
    "    no_repeat_ngram_size=3,  # Avoids repeating 3-grams\n",
    "    early_stopping=True,  # Stops if output is complete\n",
    "    pad_token_id=fine_tuned_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right. So, the text is, again, totally inaccurate. But there's a notable shift, which is the fact that writing style now resembles Wikipedia's style quite a lot. It's clear that training data was not enough to allow GPT-2 to go into conceptual depth. We can indeed use a book to fine-tune it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_utility import pdf_to_txt\n",
    "\n",
    "pdf_path = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Books\\History\\The Great War_ A Combat History of the First World War (1).pdf\"\n",
    "txt_path = r\"the_great_war_book.txt\"\n",
    "\n",
    "pdf_to_txt(pdf_path=pdf_path, txt_path=txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pdf_to_txt() which we defined in pdf_utility.py, and extract text from The Great War by Peter Hart. It's available on Internet Archive, and is a fairly in-depth exploration of WW1. (I hope I don't face copyright issues, haha.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corpus = \"the_great_war_book.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1281708\n",
      "First 500 characters:\n",
      "\n",
      "\t\tÂ£\tÂ©\tÂ½\tÂ¾\tÃ\tÃ\tÃ \tÃ¢\tÃ¤\tÃ§\tÃ¨\tÃ©\tÃª\tÃ³\tÃ´\tÃ¶\tÃ¼\tÅ\t\n",
      "Å\n",
      "\t&\tâ\tâ\tâ\tâ\tâ\tâ\tâ¦\n",
      "THE\tGREAT\tWAR\n",
      "ALSO\tBY\tPETER\tHART\n",
      "Gallipoli\n",
      "1918:\tA\tVery\tBritish\tVictory\n",
      "Aces\tFalling:\tWar\tAbove\tthe\tTrenches,\t1918\n",
      "The\tSomme\n",
      "Bloody\tApril:\tSlaughter\tin\tthe\tSkies\tOver\tArras,\t1917\n",
      "Somme\tSuccess:\tThe\tRFC\tand\tthe\tBattle\tof\tthe\tSomme\n",
      "WITH\tNIGEL\tSTEEL\n",
      "Tumult\tin\tthe\tClouds\n",
      "Defeat\tat\tGallipoli\n",
      "Passchendaele\n",
      "Jutland\t1916\n",
      "THE\tGREAT\tWAR\n",
      "A\tCombat\tHistory\tof\tthe\n",
      "First\tWorld\tWar\n",
      "PETER\tHART\n",
      "\n",
      "Oxford\tUniversity\tPress\n",
      "Oxford\tUniversity\tPress\tis\ta\tdepartm\n",
      "Number of lines: 28680\n"
     ]
    }
   ],
   "source": [
    "# A glance at the corpus from PDF.\n",
    "\n",
    "# Load the new corpus\n",
    "pdf_corpus = \"the_great_war_book.txt\"\n",
    "with open(pdf_corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "    new_corpus_text = f.read()\n",
    "\n",
    "# Basic inspection\n",
    "print(f\"Total characters: {len(new_corpus_text)}\")\n",
    "print(f\"First 500 characters:\\n{new_corpus_text[:500]}\")\n",
    "print(f\"Number of lines: {len(new_corpus_text.splitlines())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are definitely the opening few pages. Alright, same drill. Load, preprocess, train, and evaluate. Let's get it done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 28633 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the new corpus as a dataset\n",
    "dataset = load_dataset(\"text\", data_files=pdf_corpus, split=\"train\")\n",
    "\n",
    "# Filter out empty lines\n",
    "dataset = dataset.filter(lambda example: example[\"text\"].strip() != \"\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot more training samples this time. We have already removed the empty rows. Let's proceed with tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-ww1-finetuned\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is set\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data collator.\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously fine-tuned model.\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-ww1-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-ww1-finetuned-v2\",  # New directory for this version\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Fewer epochs for additional fine-tuning\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=200,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=200,\n",
    "    learning_rate=2e-5,  # Lower LR for fine-tuning an already trained model\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'd only train for 3 epochs, since our fine-tined (v1) already knows all the basics, but lacks depth. We'd use a lower learning rate, so that the model preserves previous learnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer. \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21477' max='21477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21477/21477 38:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.362500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>2.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>2.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>2.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>2.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>2.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>2.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>2.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>2.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>2.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>2.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>2.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>2.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>2.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>2.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>2.036700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-ww1-finetuned-v2\\\\tokenizer_config.json',\n",
       " './gpt2-ww1-finetuned-v2\\\\special_tokens_map.json',\n",
       " './gpt2-ww1-finetuned-v2\\\\vocab.json',\n",
       " './gpt2-ww1-finetuned-v2\\\\merges.txt',\n",
       " './gpt2-ww1-finetuned-v2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "trainer.save_model(\"./gpt2-ww1-finetuned-v2\")\n",
    "tokenizer.save_pretrained(\"./gpt2-ww1-finetuned-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process was clearly a lot more messy this time. It failed to go beyond ~2.00. But let's hope that it learned something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "World War I was\ta\tdreadful\texperience\tfor\tthe\tAustrians.\tThe\tRussians\thad\tto\tget\tover\tthem\tand\twin\ttheir\twar\ton\t31\tJuly.â\tBut\tthis\twas\tnot\tjust\tan\tenormous\twonderful,\tit\tis\tnigh-inviolable.,âTheâ\tGermans\twere\talso\tfailing\tâ\tin\twhat\tseemed\tlike\teverywhere,âââ\tuntil\tthey\thappened\tat\t12.40\tpm\twhen\tJoffre\tshowed\tup\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned v2 model and tokenizer\n",
    "fine_tuned_model_v2 = GPT2LMHeadModel.from_pretrained(\"./gpt2-ww1-finetuned-v2\")\n",
    "fine_tuned_tokenizer_v2 = GPT2Tokenizer.from_pretrained(\"./gpt2-ww1-finetuned-v2\")\n",
    "fine_tuned_model_v2.eval()\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"World War I was\"\n",
    "input_ids = fine_tuned_tokenizer_v2.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model_v2.generate(\n",
    "    input_ids,\n",
    "    max_length=150,          # Adjust for length\n",
    "    do_sample=True,          # Creative sampling\n",
    "    top_k=50,                # Top-k sampling\n",
    "    top_p=0.95,              # Top-p sampling\n",
    "    temperature=0.7,         # Balanced randomness\n",
    "    no_repeat_ngram_size=2,  # Prevent repetition\n",
    "    pad_token_id=fine_tuned_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "generated_text = fine_tuned_tokenizer_v2.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so now the model has adapted to The Great War by Peter Hart. This generated sequence is slightly less nonsensical than before. Now, why the model struggles to learn history could be due to underfitting. We used the base model, which may not be powerful enough, as it contains 124 million parameters, as compared to XL version, which comes with 1.5 billion parameters. Anyway, our key learnings from this project:\n",
    "Key Learnings\n",
    "\n",
    "1. Model Limitations\n",
    "\n",
    "We used GPT-2 Base (124M parameters), which showed limitations in capturing long-term coherence and historical accuracy.\n",
    "\n",
    "Generated text exhibited nonsensical phrasing and factual inaccuracies, likely due to the modelâs limited parameter size.\n",
    "\n",
    "The model struggled with dates, names, and logical event sequences, leading to unrealistic outputs.\n",
    "\n",
    "2. Training Process Observations\n",
    "\n",
    "Training loss steadily decreased, but validation loss was initially NaN due to improper dataset formatting.\n",
    "\n",
    "The model started to produce less chaotic but still flawed generations after multiple iterations.\n",
    "\n",
    "Larger context windows improved text structure, but did not significantly enhance factual accuracy.\n",
    "\n",
    "3. Data Preprocessing Challenges\n",
    "\n",
    "The dataset contained empty or malformed samples, which affected training stability.\n",
    "\n",
    "Special characters, inconsistent formatting, and line breaks caused tokenization issues.\n",
    "\n",
    "Some historical terms and dates were not properly learned due to inconsistent representation.\n",
    "\n",
    "Areas for Improvement\n",
    "\n",
    "1. Upgrade Model Size\n",
    "\n",
    "Fine-tuning GPT-2 Medium (355M) or Large (774M) can improve coherence and factual consistency.\n",
    "\n",
    "Consider switching to LLama 2 or Falcon, which are more efficient and powerful than GPT-2.\n",
    "\n",
    "2. Better Training Strategy\n",
    "\n",
    "Use more epochs, but carefully monitor for overfitting.\n",
    "\n",
    "Implement LoRA/QLoRA for parameter-efficient fine-tuning.\n",
    "\n",
    "Experiment with learning rate scheduling for better convergence.\n",
    "\n",
    "3. Enhanced Preprocessing\n",
    "\n",
    "Clean dataset by removing empty samples and malformed text.\n",
    "\n",
    "Standardize date and name formats to help with consistency.\n",
    "\n",
    "Improve tokenization handling for special characters.\n",
    "\n",
    "4. Alternative Sampling Methods\n",
    "\n",
    "Experiment with different temperature, top-k, and top-p values for more controlled text generation.\n",
    "\n",
    "Implement beam search instead of sampling for more structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
